{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not \"venv\" in sys.executable:\n",
    "    raise EnvironmentError(\"The script must be run in a virtual environment with ipykernel installed\")\n",
    "\n",
    "user_input = input(\"(y/n) Would you like to uninstall all existing packages and install the necessary requirements from startingreqs.txt?\")\n",
    "\n",
    "if user_input.lower() == \"y\":\n",
    "    #saves current packages to uninstall.txt\n",
    "    !pip3 freeze > uninstall.txt\n",
    "    #uninstalls all non-essential packages in environment\n",
    "    !pip3 uninstall -r uninstall.txt -y\n",
    "    #installs necessary packages\n",
    "    !pip3 install -r additionalreqs.txt\n",
    "\n",
    "import pprint\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Podcasts/podcastsegmentfr.mp3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gets path of first audio file in podcast folder, excludes .ds_store\n",
    "podcastfolder = \"./Podcasts\"\n",
    "podcastfiles = []\n",
    "for file in os.listdir(podcastfolder):\n",
    "    if file != \".DS_Store\":\n",
    "        podcastfiles.append(file)\n",
    "podcastaudio = podcastfolder + \"/\" + podcastfiles[0]\n",
    "podcastaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline = input(\"Run the baseline model for comparison? Will need to install ffmpeg (https://ffmpeg.org/) (y/n)\")\n",
    "if test_baseline.lower() == \"y\":\n",
    "    import whisper\n",
    "    baseline_model_name = \"turbo\"\n",
    "    baseline_model = whisper.load_model(baseline_model_name, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael.781/Desktop/DataScience/venvMT/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" sa capacité à fédérer, c'est-à-dire à faire en sorte que la population se réunisse et soit heureuse ensemble. La France a aussi pu montrer, grâce \"\n",
      " 'aux Jeux Olympiques, sa capacité à construire des infrastructures, donc des bâtiments de qualité, à accueillir le tourisme et à organiser en '\n",
      " \"général un des événements les plus importants du monde. Voilà, donc ça c'est pour la partie un peu logistique. Et maintenant, si on s'intéresse \"\n",
      " 'plus particulièrement aux performances sportives, la France avait un objectif assez clair pour ces Jeux Olympiques. On voulait faire partie du top '\n",
      " '5 des nations au classement des médailles. Autrement dit, être dans les 5 premiers pays en ce qui concerne le nombre de médailles et plus '\n",
      " \"particulièrement les médailles d'or. Et Cocorico, on a réussi, puisque la France a remporté 64.\")\n"
     ]
    }
   ],
   "source": [
    "if test_baseline.lower() == \"y\":\n",
    "    result = baseline_model.transcribe(podcastaudio, beam_size=5, language=\"fr\")\n",
    "    transcript = result[\"text\"]\n",
    "    pprint.pprint(transcript, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michael.781/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#CHANGE FOLLOWING TO TAKE TRANSCRIPT FROM TRANSCRIBE FILE\n",
    "nltk.download('punkt_tab')\n",
    "sentences = nltk.tokenize.sent_tokenize(transcript_from_other, language='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael.781/Desktop/DataScience/venvMT/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translation = []\n",
    "for sentence in sentences:\n",
    "    input = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "    output = model.generate(**input)\n",
    "    translation.append([tokenizer.decode(t, skip_special_tokens=True) for t in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael.781/Desktop/DataScience/venvMT/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# mt_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "# tokenizer = MarianTokenizer.from_pretrained(mt_model_name)\n",
    "# mt_model = MarianMTModel.from_pretrained(mt_model_name)\n",
    "\n",
    "\n",
    "# transcript_sentences = nltk.sent_tokenize(transcript, language=\"french\")\n",
    "# tokenized_sentences = []\n",
    "# for sentence in transcript_sentences:\n",
    "#     tokenized_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
    "#     tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "# translation = []\n",
    "# for sentence in tokenized_sentences:\n",
    "#     input = {\"input_ids\": sentence[\"input_ids\"][0].unsqueeze(0)}\n",
    "#     output = mt_model.generate(**input)\n",
    "#     translation.append(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its ability to unite, that is to say, to ensure that the population gathers and is happy together.\n",
      "France was also able to demonstrate, thanks to the Olympic Games, its capacity to build infrastructure, thus quality buildings, to host tourism and to organize in general one of the most important events in the world.\n",
      "That's it, so that's for the logistical part.\n",
      "And now, if we are particularly interested in sports performances, France had a clear objective for these Olympic Games.\n",
      "We wanted to be one of the top five nations in the medal rankings.\n",
      "In other words, to be in the top 5 countries with regard to the number of medals and more particularly gold medals.\n",
      "And Cocorico, we made it, since France won 64.\n"
     ]
    }
   ],
   "source": [
    "for sentence in translation:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
